---
title: Analyze Application Insight logs with Spark on HDInsight | Microsoft Azure
description: Learn how to export Application Insight logs to blob storage, and then analyze the logs with Spark on HDInsight.
services: hdinsight
documentationcenter: ''
author: Blackmist
manager: jhubbard
editor: cgronlun

ms.service: hdinsight
ms.devlang: na
ms.topic: article
ms.tgt_pltfrm: na
ms.workload: big-data
ms.date: 08/15/2016
ms.author: larryfr

---
# Analyze Application Insights telemetry logs with Spark on HDInsight
[Visual Studio Application Insights](../application-insights/app-insights-overview.md) is an analytics service that monitors your web applications. Telemetry data generated by Application Insights can be exported to Azure Storage, and from there it can be analyzed by HDInsight.

In this document, you will learn how to use HDInsight to analyze Application Insight telemetry data using Apache Spark.

## Prerequisites
* An Azure subscription.
* An application that is configured to use Application Insights. 
* Familiarity with creating a Linux-based HDInsight cluster. If you are not familiar with creating a cluster, see [Create Spark on HDInsight](hdinsight-apache-spark-jupyter-spark-sql.md) for more information.
  
  > [!NOTE]
  > This document does not provide a walk-through of creating a new cluster. Instead, it references other documents that provide information on how to create a cluster that can access the telemetry data.
  > 
  > 
* A web browser. This is used to interactively run analysis using a Jupyter Notebook.

The following were used in developing and testing this document:

* Application Insights telemetry data was generated using a [Node.js web app configured to use Application Insights](../application-insights/app-insights-nodejs.md).
* A Linux-based Spark on HDInsight cluster version 3.4 was used to analyze the data.

## Architecture and planning
The following diagram illustrates the service architecture of this example:

![diagram showing data flowing from Application Insights to blob storage, then being processed by Spark on HDInsight](./media/hdinsight-spark-analyze-application-insight-logs/appinsightshdinsight.png)

### Azure storage
An HDInsight cluster can directly access block blobs from an Azure storage account, and Application Insights can be configured to continuously export telemetry information to blobs in Azure storage. However, there are some requirements that you must follow:

* **Location**: The storage account should be located in the same region as HDInsight. This reduces latency when accessing the data, and avoids egress charges that occur when you move data between regions.
* **Blob type**: HDInsight only supports block blobs. Application Insights defaults to using block blobs, so should work by default with HDInsight.
* **Access permissions**: If you use the same storage account for both Application Insights continuous export and HDInsight's default storage, HDInsight has full access to the Application Insight telemetry data. This means that it is possible to delete the telemetry data from the HDInsight cluster.
  
    Instead, it is recommended that you use separate storage accounts for HDInsight and Application Insights telemetry, and [use Shared Access Signatures (SAS) to restrict access to the data from HDInsight](hdinsight-storage-sharedaccesssignature-permissions.md). Using an SAS allows you to grant HDInsight read-only access to the telemetry data.

### Data schema
Application Insights provides [export data model](../application-insights/app-insights-export-data-model.md) information for the telemetry data format exported to blobs. The steps in this document use Spark SQL to work with the data. Spark SQL can automatically generate a schema for the JSON data structure logged by Application Insights, so you do not have to manually define the schema when performing analysis.

## Export telemetry data
Follow the steps in [Configure Continuous Export](../application-insights/app-insights-export-telemetry.md) to configure your Application Insights to export telemetry information to an Azure storage blob.

## Configure HDInsight to access the data
Use the information in [Use Shared Access Signatures (SAS) to restrict access to the data from HDInsight](hdinsight-storage-sharedaccesssignature-permissions.md) to create a SAS for the blob container that holds the exported telemetry data. The SAS should provide read-only access to the data.

The Shared Access Signature document provides information on how you can add the SAS storage to an existing Linux-based HDInsight cluster. It also provides information on how to add it when creating a new HDInsight cluster.

## Analyze the data using Python (PySpark)
1. From the [Azure portal](https://portal.azure.com), select your Spark on HDInsight cluster. From the **Quick Links** section, select **Cluster Dashboards**, and then select **Jupyter Notebook** from the Cluster Dashboard__ blade.
   
    ![The cluster dashboards](./media/hdinsight-spark-analyze-application-insight-logs/clusterdashboards.png)
2. In the upper right corner of the Jupyter page, select **New**, and then **PySpark**. This opens a new browser tab containing a Python-based Jupyter Notebook.
3. In the first field (called a **cell**) on the page, enter the following:
   
        sc._jsc.hadoopConfiguration().set('mapreduce.input.fileinputformat.input.dir.recursive', 'true')
   
    This allows Spark to recursively access the directory structure for the input data. Application Insights telemetry is logged to a directory structure similar to the following:
   
        /{telemetry type}/YYYY-MM-DD/{##}/
4. Use **SHIFT+ENTER** to run the code. On the left side of the cell, an '\*' will appear between the brackets to indicate that the code in this cell is being executed. Once it completes, the '\*' will change to a number, and output similar to the following will be displayed below the cell:
   
        Creating SparkContext as 'sc'
   
        ID    YARN Application ID    Kind    State    Spark UI    Driver log    Current session?
        3    application_1468969497124_0001    pyspark    idle    Link    Link    âœ”
   
        Creating HiveContext as 'sqlContext'
        SparkContext and HiveContext created. Executing user code ...
5. A new cell will have been created below the first one. Enter the following in the new cell. Replace **CONTAINER** and **STORAGEACCOUNT** with the Azure storage account name and blob container name that you used when configuring the Application Insights continuous export.
   
        %%bash
        hdfs dfs -ls wasb://CONTAINER@STORAGEACCOUNT.blob.core.windows.net/
   
    Use **SHIFT+ENTER** to execute this cell. You will see a result similar to the following:
   
        Found 1 items
        drwxrwxrwx   -          0 1970-01-01 00:00 wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_2bededa61bc741fbdee6b556571a4831
   
    The wasb path returned is the location of the Application Insights telemetry data. Change the `hdfs dfs -ls` line in the cell to use the wasb path returned, and then use **SHIFT+ENTER** to run the cell again. This time, the results should display the directories that contain telemetry data.
   
   > [!NOTE]
   > For the remainder of the steps in this section, the `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` directory was used. This directory may not exist unless your telemetry data is for a web app. If you are using telemetry data that does not include a requests directory, pick another directory and adjust the rest of the steps to use that directory and the schema for the data stored within it.
   > 
   > 
6. In the next cell, enter the following. Replace **WASB\_PATH** with the path from the previous step.
   
        jsonFiles = sc.textFile('WASB_PATH')
        jsonData = sqlContext.read.json(jsonFiles)
   
    This creates a new dataframe from the JSON files exported by the continuous export process. Use **SHIFT+ENTER** to run this cell.
7. In the next cell, enter and run the following to view the schema that Spark created for the JSON files:
   
        jsonData.printSchema()
   
    The schema for each type of telemetry will be different. The following is the schema that is generated for web requests (data stored in the `Requests` subdirectory):
   
        root
        |-- context: struct (nullable = true)
        |    |-- application: struct (nullable = true)
        |    |    |-- version: string (nullable = true)
        |    |-- custom: struct (nullable = true)
        |    |    |-- dimensions: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |    |-- metrics: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- eventTime: string (nullable = true)
        |    |    |-- isSynthetic: boolean (nullable = true)
        |    |    |-- samplingRate: double (nullable = true)
        |    |    |-- syntheticSource: string (nullable = true)
        |    |-- device: struct (nullable = true)
        |    |    |-- browser: string (nullable = true)
        |    |    |-- browserVersion: string (nullable = true)
        |    |    |-- deviceModel: string (nullable = true)
        |    |    |-- deviceName: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- osVersion: string (nullable = true)
        |    |    |-- type: string (nullable = true)
        |    |-- location: struct (nullable = true)
        |    |    |-- city: string (nullable = true)
        |    |    |-- clientip: string (nullable = true)
        |    |    |-- continent: string (nullable = true)
        |    |    |-- country: string (nullable = true)
        |    |    |-- province: string (nullable = true)
        |    |-- operation: struct (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |-- session: struct (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- isFirst: boolean (nullable = true)
        |    |-- user: struct (nullable = true)
        |    |    |-- anonId: string (nullable = true)
        |    |    |-- isAuthenticated: boolean (nullable = true)
        |-- internal: struct (nullable = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- documentVersion: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |-- request: array (nullable = true)
        |    |-- element: struct (containsNull = true)
        |    |    |-- count: long (nullable = true)
        |    |    |-- durationMetric: struct (nullable = true)
        |    |    |    |-- count: double (nullable = true)
        |    |    |    |-- max: double (nullable = true)
        |    |    |    |-- min: double (nullable = true)
        |    |    |    |-- sampledValue: double (nullable = true)
        |    |    |    |-- stdDev: double (nullable = true)
        |    |    |    |-- value: double (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |    |-- responseCode: long (nullable = true)
        |    |    |-- success: boolean (nullable = true)
        |    |    |-- url: string (nullable = true)
        |    |    |-- urlData: struct (nullable = true)
        |    |    |    |-- base: string (nullable = true)
        |    |    |    |-- hashTag: string (nullable = true)
        |    |    |    |-- host: string (nullable = true)
        |    |    |    |-- protocol: string (nullable = true)
8. Use the following to register the dataframe as a temporary table and run a query against the data:
   
        jsonData.registerTempTable("requests")
        sqlContext.sql("select context.location.city from requests where context.location.city is not null")
   
    This query will return the city information for the top 20 records where context.location.city is not null.
   
   > [!NOTE]
   > The context structure is present in all telemetry logged by Application Insights; however, the city element may not be populated in your logs. Use the schema to identify other elements that you can query that may contain data for your logs.
   > 
   > 
   
    This query will return information similar to the following:
   
        +---------+
        |     city|
        +---------+
        | Bellevue|
        |  Redmond|
        |  Seattle|
        |Charlotte|
        ...
        +---------+

## Analyze the data using Scala
1. From the [Azure portal](https://portal.azure.com), select your Spark on HDInsight cluster. From the **Quick Links** section, select **Cluster Dashboards**, and then select **Jupyter Notebook** from the Cluster Dashboard__ blade.
   
    ![The cluster dashboards](./media/hdinsight-spark-analyze-application-insight-logs/clusterdashboards.png)
2. In the upper right corner of the Jupyter page, select **New**, and then **Scala**. This will open a new browser tab containing a Scala-based Jupyter Notebook.
3. In the first field (called a **cell**) on the page, enter the following:
   
        sc.hadoopConfiguration.set("mapreduce.input.fileinputformat.input.dir.recursive", "true")
   
    This allows Spark to recursively access the directory structure for the input data. Application Insights telemetry is logged to a directory structure similar to the following:
   
        /{telemetry type}/YYYY-MM-DD/{##}/
4. Use **SHIFT+ENTER** to run the code. On the left side of the cell, an '\*' will appear between the brackets to indicate that the code in this cell is being executed. Once it completes, the '\*' will change to a number, and output similar to the following will be displayed below the cell:
   
        Creating SparkContext as 'sc'
   
        ID    YARN Application ID    Kind    State    Spark UI    Driver log    Current session?
        3    application_1468969497124_0001    spark    idle    Link    Link    âœ”
   
        Creating HiveContext as 'sqlContext'
        SparkContext and HiveContext created. Executing user code ...
5. A new cell will have been created below the first one. Enter the following in the new cell. Replace **CONTAINER** and **STORAGEACCOUNT** with the Azure storage account name and blob container name that you used when configuring the Application Insights continuous export.
   
        %%bash
        hdfs dfs -ls wasb://CONTAINER@STORAGEACCOUNT.blob.core.windows.net/
   
    Use **SHIFT+ENTER** to execute this cell. You will see a result similar to the following:
   
        Found 1 items
        drwxrwxrwx   -          0 1970-01-01 00:00 wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_2bededa61bc741fbdee6b556571a4831
   
    The wasb path returned is the location of the Application Insights telemetry data. Change the `hdfs dfs -ls` line in the cell to use the wasb path returned, and then use **SHIFT+ENTER** to run the cell again. This time, the results should display the directories that contain telemetry data.
   
   > [!NOTE]
   > For the remainder of the steps in this section, the `wasb://appinsights@contosostore.blob.core.windows.net/contosoappinsights_{ID}/Requests` directory was used. This directory may not exist unless your telemetry data is for a web app. If you are using telemetry data that does not include a requests directory, pick another directory and adjust the rest of the steps to use that directory and the schema for the data stored within it.
   > 
   > 
6. In the next cell, enter the following. Replace **WASB\_PATH** with the path from the previous step.
   
        jsonFiles = sc.textFile('WASB_PATH')
        jsonData = sqlContext.read.json(jsonFiles)
   
    This creates a new dataframe from the JSON files exported by the continuous export process. Use **SHIFT+ENTER** to run this cell.
7. In the next cell, enter and run the following to view the schema that Spark created for the JSON files:
   
        jsonData.printSchema
   
    The schema for each type of telemetry will be different. The following is the schema that is generated for web requests (data stored in the `Requests` subdirectory):
   
        root
        |-- context: struct (nullable = true)
        |    |-- application: struct (nullable = true)
        |    |    |-- version: string (nullable = true)
        |    |-- custom: struct (nullable = true)
        |    |    |-- dimensions: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |    |-- metrics: array (nullable = true)
        |    |    |    |-- element: string (containsNull = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- eventTime: string (nullable = true)
        |    |    |-- isSynthetic: boolean (nullable = true)
        |    |    |-- samplingRate: double (nullable = true)
        |    |    |-- syntheticSource: string (nullable = true)
        |    |-- device: struct (nullable = true)
        |    |    |-- browser: string (nullable = true)
        |    |    |-- browserVersion: string (nullable = true)
        |    |    |-- deviceModel: string (nullable = true)
        |    |    |-- deviceName: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- osVersion: string (nullable = true)
        |    |    |-- type: string (nullable = true)
        |    |-- location: struct (nullable = true)
        |    |    |-- city: string (nullable = true)
        |    |    |-- clientip: string (nullable = true)
        |    |    |-- continent: string (nullable = true)
        |    |    |-- country: string (nullable = true)
        |    |    |-- province: string (nullable = true)
        |    |-- operation: struct (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |-- session: struct (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- isFirst: boolean (nullable = true)
        |    |-- user: struct (nullable = true)
        |    |    |-- anonId: string (nullable = true)
        |    |    |-- isAuthenticated: boolean (nullable = true)
        |-- internal: struct (nullable = true)
        |    |-- data: struct (nullable = true)
        |    |    |-- documentVersion: string (nullable = true)
        |    |    |-- id: string (nullable = true)
        |-- request: array (nullable = true)
        |    |-- element: struct (containsNull = true)
        |    |    |-- count: long (nullable = true)
        |    |    |-- durationMetric: struct (nullable = true)
        |    |    |    |-- count: double (nullable = true)
        |    |    |    |-- max: double (nullable = true)
        |    |    |    |-- min: double (nullable = true)
        |    |    |    |-- sampledValue: double (nullable = true)
        |    |    |    |-- stdDev: double (nullable = true)
        |    |    |    |-- value: double (nullable = true)
        |    |    |-- id: string (nullable = true)
        |    |    |-- name: string (nullable = true)
        |    |    |-- responseCode: long (nullable = true)
        |    |    |-- success: boolean (nullable = true)
        |    |    |-- url: string (nullable = true)
        |    |    |-- urlData: struct (nullable = true)
        |    |    |    |-- base: string (nullable = true)
        |    |    |    |-- hashTag: string (nullable = true)
        |    |    |    |-- host: string (nullable = true)
        |    |    |    |-- protocol: string (nullable = true)
8. Use the following to register the dataframe as a temporary table and run a query against the data:
   
        jsonData.registerTempTable("requests")
        var city = sqlContext.sql("select context.location.city from requests where context.location.city is not null limit 10").show()
   
    This query will return the city information for the top 20 records where context.location.city is not null.
   
   > [!NOTE]
   > The context structure is present in all telemetry logged by Application Insights; however, the city element may not be populated in your logs. Use the schema to identify other elements that you can query that may contain data for your logs.
   > 
   > 
   
    This query will return information similar to the following:
   
        +---------+
        |     city|
        +---------+
        | Bellevue|
        |  Redmond|
        |  Seattle|
        |Charlotte|
        ...
        +---------+

## Next steps
For more examples of using Spark to work with data and services in Azure, see the following documents:

* [Spark with BI: Perform interactive data analysis using Spark in HDInsight with BI tools](hdinsight-apache-spark-use-bi-tools.md)
* [Spark with Machine Learning: Use Spark in HDInsight for analyzing building temperature using HVAC data](hdinsight-apache-spark-ipython-notebook-machine-learning.md)
* [Spark with Machine Learning: Use Spark in HDInsight to predict food inspection results](hdinsight-apache-spark-machine-learning-mllib-ipython.md)
* [Spark Streaming: Use Spark in HDInsight for building real-time streaming applications](hdinsight-apache-spark-eventhub-streaming.md)
* [Website log analysis using Spark in HDInsight](hdinsight-apache-spark-custom-library-website-log-analysis.md)

For information on creating and running Spark applications, see the following documents:

* [Create a standalone application using Scala](hdinsight-apache-spark-create-standalone-application.md)
* [Run jobs remotely on a Spark cluster using Livy](hdinsight-apache-spark-livy-rest-interface.md)

